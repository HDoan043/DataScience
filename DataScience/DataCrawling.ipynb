{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb7480a",
   "metadata": {},
   "source": [
    "# **DATA CRAWLING WITH PYTHON**\n",
    "<BR><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c6bea",
   "metadata": {},
   "source": [
    "## **SET UP ENVIRONMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (4.36.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.30.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.30.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: openpyxl in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl) (2.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: chromedriver-autoinstaller in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from chromedriver-autoinstaller) (24.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lxml in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (6.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: PyYAML in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (6.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install cloudscraper                   \n",
    "%pip install playwright\n",
    "%pip install beautifulsoup4             \n",
    "%pip install pandas \n",
    "%pip install openpyxl \n",
    "%pip install chromedriver-autoinstaller\n",
    "%pip install lxml\n",
    "%pip install PyYAML\n",
    "%pip install requests\n",
    "%pip install numpy\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92729a6",
   "metadata": {},
   "source": [
    "## **CRAWLING DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2078c",
   "metadata": {},
   "source": [
    "---\n",
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json                                 # Handle JSON data\n",
    "import time                                 # Create latencies between 2 requests to avoid being blocked\n",
    "import re                                   # Use regular expression( biểu thức chính quy) to processing strings\n",
    "\n",
    "import requests                             # Send HTTP requests to fetch web content\n",
    "from bs4 import BeautifulSoup               # Parse HTTP and extract information\n",
    "from tqdm import tqdm                       # Display progress bar\n",
    "\n",
    "import yaml                                 # Read configuration files\n",
    "from typing import Dict, List               # Provide type hints for cleaner code\n",
    "from urllib.parse import urljoin, urlparse # Handle urls\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2445e88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Connect the website and receive HTML response**\n",
    "\n",
    "For this step, we have to define functions to send requests to website and receive HTML response. However, there are some barriers of website preventing the requests to avoid automatically crawling by bots, which can make server overload. The functions should overcome this limitation to crawl data politely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8a157",
   "metadata": {},
   "source": [
    "**Sending HTTP requests**\n",
    "\n",
    "A HTTP request contains 3 parts:\n",
    "* *`Request Line`*: includes:\n",
    "    * Method: `GET` to request data, `POST` to send data\n",
    "    * URI: Path to target resource\n",
    "    * HTTP Version\n",
    "\n",
    "* *`Heading`*: includes additional information which are type of data, language, where the data comes from, verification information,...\n",
    "\n",
    "* *`Body`*: includes content that users want to send to server( from forms or files attached)\n",
    "\n",
    "The anti-detection data crawling technique will create a HTTP request that looks like coming from normal groups of users but not from a bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27132fc",
   "metadata": {},
   "source": [
    "**Anti-detection Techniques**\n",
    "\n",
    "* ***User-agent rotation***: rotate User-Agent string to mimic different browser( thay đổi User-Agent để bắt chước các trình duyệt khác):\n",
    "    * Field User-agent in the Header part of a HTTP request indicates the config of the hardware sending messages\n",
    "\n",
    "* ***Browser-like headers***: Use headers that resemble a real browser( sử dụng headers giống trình duyệt thực)\n",
    "\n",
    "* ***Random delay***: Randomly delay between 2 requests\n",
    "\n",
    "* ***Retry mechanism***: Retry up to 3 times on failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb694d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, domain_name = 'https://batdongsan.com.vn/'):\n",
    "    '''\n",
    "    Function gets the HTML document from an url with anti-detection techniques\n",
    "    '''\n",
    "    import random\n",
    "    import requests\n",
    "\n",
    "    if not url.startswith('http'):\n",
    "        return None \n",
    "    \n",
    "    # list of user-agent that mimics the behaviors of real users\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "\n",
    "    # create headers which simulate the real header, randomly using the user-agents created\n",
    "    headers = {\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7,fr;q=0.6',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "        'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'User-Agent': random.choice(user_agents),\n",
    "        'Referer': domain_name,\n",
    "        'Origin': domain_name\n",
    "    }\n",
    "\n",
    "    # Create a new session to maintain cookies and headers accross multiple requests\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Update session's header by above defined header\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    # Try to request up to 3 times in case of failure\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            time.sleep(random.uniform(1,3))\n",
    "\n",
    "            response = session.get(\n",
    "                url,\n",
    "                timeout=30,\n",
    "                allow_redirects=True,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8' # using utf-8 for encoding Vietnamese\n",
    "                html_text = response.text\n",
    "                response.close()\n",
    "                session.close()\n",
    "                return html_text\n",
    "            \n",
    "            # If request is denined\n",
    "            elif response.status_code == 403:\n",
    "                # Try to change user-agent and retry\n",
    "                headers['User-Agent'] = random.choice(user_agents)\n",
    "                session.headers.update(headers)\n",
    "                time.sleep(random.uniform(2,5))                     # Wait for 2-5 seconds\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                if attempt == 2:\n",
    "                    print(\"Request to {} is deninded.\".format(url))\n",
    "                response.close()\n",
    "\n",
    "        # If SSL Error --> Try again\n",
    "        except requests.exceptions.SSLError:\n",
    "            continue\n",
    "\n",
    "        # If the request does not receive any responses due to general errors: time out, connection fail,...\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == 2:\n",
    "                print(\"Request error for {}: {}\".format(url, e))\n",
    "        \n",
    "        # Catch other exception else\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                print(\"Unknown error for {}: {}\".format(url, e))\n",
    "\n",
    "        # Increase delay time between retries to avoid being blocked by server\n",
    "        if attempt < 2:\n",
    "            time.sleep(random.uniform( 3 + attempt, 6 + attempt))\n",
    "\n",
    "    session.close()     # end sesion\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375537b",
   "metadata": {},
   "source": [
    "**Advanced anti-detection techniques**\n",
    "\n",
    "Sometimes, websites may block the Python requests but still allow access from other tools such as `cURL`. These tools are similar with `requests` of Python, they help to send requests and receive response from webs. Therefore, we need a backup method in case of Python requests sent by function `get_html` being blocked.\n",
    "\n",
    "`cURL` is a CLI tools, which only provides instructions on CLI instead of a library as `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_curl(url, domain_name = 'https://batdongsan.com.vn/'):\n",
    "    '''\n",
    "    In case of Python requests is blocked by website,\n",
    "    This function will try to send requests by cURL instead of Python requests\n",
    "    '''\n",
    "    import subprocess # Use to run cURL in code, which only provides instruction through CLI\n",
    "    import tempfile   # Use to create temporary files\n",
    "    import os\n",
    "\n",
    "    if not url.startswith('http'):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create a temporary file to store the downloaded HTML\n",
    "        with tempfile.NamedTemporaryFile(mode = 'w+', delete=False, suffix='.html') as temp_file:\n",
    "            temp_filename = temp_file.name\n",
    "\n",
    "        # Build cURL to simulate a read browser\n",
    "        curl_command = [\n",
    "            'curl',\n",
    "            '-s',\n",
    "            '-L',\n",
    "            '--compressed',\n",
    "            '--max-time', '30',\n",
    "            '--retry', '2',\n",
    "             '--user-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            # Use a realistic User-Agent to avoid detection (Giả lập trình duyệt thật để tránh bị chặn)\n",
    "\n",
    "            # Common browser headers (Các header phổ biến mà trình duyệt gửi kèm)\n",
    "            '--header', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            '--header', 'Accept-Language: vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            '--header', 'Accept-Encoding: gzip, deflate, br',\n",
    "            '--header', 'DNT: 1',  # Do Not Track header (Yêu cầu không theo dõi)\n",
    "            '--header', 'Connection: keep-alive',\n",
    "            '--header', 'Upgrade-Insecure-Requests: 1',\n",
    "            '--header', 'Sec-Fetch-Dest: document',\n",
    "            '--header', 'Sec-Fetch-Mode: navigate',\n",
    "            '--header', 'Sec-Fetch-Site: none',\n",
    "            '--header', 'Sec-Fetch-User: ?1',\n",
    "            '--header', 'Cache-Control: max-age=0',\n",
    "            '--referer', domain_name,  # Pretend to come from vnexpress homepage (Giả vờ người dùng click từ trang chủ)\n",
    "            '--output', temp_filename,              # Save output HTML to temp file (Ghi nội dung tải về vào file tạm)\n",
    "            url\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(\n",
    "            curl_command,\n",
    "            capture_output = True,\n",
    "            text = True, \n",
    "            timeout = 35\n",
    "        )\n",
    "\n",
    "        # If sucessfully receive response: response is saved in temp_filename\n",
    "        if result.returncode == 0:\n",
    "            # Extract content saved in temporary file\n",
    "            with open(temp_filename, 'r', encoding = 'utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Remove the temp file after reading\n",
    "            os.unlink(temp_filename)\n",
    "\n",
    "            # Check if the content is long enough\n",
    "            if len(content) > 3000:\n",
    "                return content\n",
    "        \n",
    "        # If temp file still exists, delete it\n",
    "        if os.path.exists(temp_filename):\n",
    "            os.unlink(temp_filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Curl method failed for {}: {}\".format(url, e))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c78bc",
   "metadata": {},
   "source": [
    "**Other Advanced anti-detection technique**\n",
    "\n",
    "Sometimes, the website uses a waiting page to verify the real action of browsers. When the website receives a request, a middle service like Cloudflare is used to bring a challenge to verify before redirect to the main website. For this challenge, we will use `playwright` to control the browser to pass the challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    install chrome driver \n",
    "    this driver is an embedded browser containing fully apis of chrome\n",
    "    this is independent with the browser chrome in local computer\n",
    "    the library playwright will use the chrominum embeded instead of the browser in local\n",
    "''' \n",
    "%playwright install chrominum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import random, time\n",
    "\n",
    "class AccessWebsite():\n",
    "    def __init__(self, page, browser):\n",
    "        self.page = page\n",
    "        self.behaviours_ls = [\n",
    "            self.human_delay,\n",
    "            self.human_move,\n",
    "            self.human_scroll,\n",
    "            self.human_wait_timeout,\n",
    "            self.human_random_click\n",
    "        ]\n",
    "\n",
    "    def human_delay(self, start=1.0, end=3.0):\n",
    "        '''\n",
    "        Method to mimic the human delay, which is usually random\n",
    "        '''\n",
    "        time.sleep(random.uniform(start, end))\n",
    "\n",
    "    def human_move(self):\n",
    "        '''\n",
    "        Method to mimic the mouse moving\n",
    "        '''\n",
    "        self.page.mouse.move(random.randint(100,400), random.randint(100,400))\n",
    "    \n",
    "    def human_scroll(self):\n",
    "        '''\n",
    "        Method to mimic the mouse scrolling\n",
    "        '''\n",
    "        length_scroll = random.randint(100, 500)\n",
    "        self.page.wheel(0, length_scroll)\n",
    "\n",
    "    def human_wait_timeout(self):\n",
    "        '''\n",
    "        Method to mimic the time waiting\n",
    "        '''\n",
    "        self.page.wait_for_timeout(random.randint(300, 1200))\n",
    "\n",
    "    def human_random_click(self):\n",
    "        '''\n",
    "        Method to mimic the random click\n",
    "        '''\n",
    "        x, y = random.randint(100, 900), random.randint(100, 600)\n",
    "        self.page.mouse.click(x, y)\n",
    "\n",
    "    def simulate_human_behaviours(self):\n",
    "        '''\n",
    "        Method to mimic the human behaviour in general\n",
    "        '''\n",
    "        num_of_choosen_behaviours = random.randint(1, 5)\n",
    "        behaviours_ls = []\n",
    "        for _ in range(num_of_choosen_behaviours):\n",
    "            rand_behaviour = random.randint(0,4)\n",
    "            behaviours_ls.append(self.behaviours_ls[rand_behaviour])\n",
    "\n",
    "        for method in behaviours_ls:\n",
    "            method()\n",
    "\n",
    "    def get_html(self, url:str):\n",
    "        '''\n",
    "        Method to access the url and get html document\n",
    "        '''\n",
    "        try:\n",
    "            self.page.goto(url)\n",
    "            self.simulate_human_behaviours()\n",
    "            html_text = self.page.content()\n",
    "            return html_text\n",
    "        except Exception as e:\n",
    "            print(\"!!! |X| Exception in using playwright to open browser: {}\".format(e))\n",
    "            return None\n",
    "\n",
    "def open_browser():\n",
    "    '''\n",
    "    Open browser --> return browser and page_controller\n",
    "    This should be called once\n",
    "    '''\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=False) # headless = False to hide browser window, True if show\n",
    "            page = browser.new_page()\n",
    "            return browser, page\n",
    "    except Exception as e:\n",
    "        print(\"!!! |X| Exception in using playwright to open browser: {}\".format(e))\n",
    "        return None\n",
    "\n",
    "def close_browser(browser):\n",
    "    '''\n",
    "    Close browser\n",
    "    '''\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f484f",
   "metadata": {},
   "source": [
    "Playwright for simulating the behaviour of using browser is sometimes heavy. There is an other library which is more efficient than Playwright, but only for passing Cloudflare: `cloudscaper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6191b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_pass_cloudflare(\n",
    "        url: str, \n",
    "        state_file: str = 'cookies.json',\n",
    "        domain_name = 'https://batdongsan.com.vn/') -> str:\n",
    "    '''\n",
    "    Function to get html document text --> Return: html text\n",
    "    '''\n",
    "    import cloudscraper, random, time, json, os, requests\n",
    "    if not url.startswith('http'):\n",
    "        return None \n",
    "    \n",
    "    # list of user-agent that mimics the behaviors of real users\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "\n",
    "    # create headers which simulate the real header, randomly using the user-agents created\n",
    "    headers = {\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7,fr;q=0.6',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "        'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'User-Agent': random.choice(user_agents),\n",
    "        'Referer': domain_name,\n",
    "        'Origin': domain_name\n",
    "    }\n",
    "\n",
    "    # Load cookies\n",
    "    if state_file:\n",
    "        with open(state_file, \"r\") as f:\n",
    "            cookies = json.load(f)\n",
    "            scraper.cookies.update(cookies)\n",
    "    \n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    # Try up to 3 times if response is not accepted\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            time.sleep(random.uniform(1,3))\n",
    "\n",
    "            response = scraper.get(url, headers = headers, timeout = random.randint(100,300))\n",
    "\n",
    "            # save cookies\n",
    "            if state_file:\n",
    "                with open(state_file, \"w\") as f:\n",
    "                    json.dump(requests.utils.dict_from_cookiejar(scraper.cookies), f)\n",
    "        \n",
    "            # Check response status\n",
    "            if response.status_code in (429, 503) or \"just a moment\" in response.text.lower() or response.text.lower():\n",
    "                # Try to use other user-agent\n",
    "                headers['User-Agent'] = random.choice(user_agents)\n",
    "                time.sleep(random.uniform(2,5))\n",
    "                continue\n",
    "            \n",
    "            elif response.status_code == 200:\n",
    "                html_text = response.text\n",
    "                return html_text\n",
    "            \n",
    "            elif attempt == 2:\n",
    "                print(\"[|X| FAIL] Cloudflare block this request\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[!!!| EXCEPTION] Exception : {}\".format(e))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_combine(url, domain_name = 'https://batdongsan.com.vn/'):\n",
    "    '''\n",
    "    This function is responsible for getting html response from websites by combination of both above functions\n",
    "    '''\n",
    "    content = get_html(url, domain_name=domain_name)\n",
    "    if content:\n",
    "        return content\n",
    "    \n",
    "    print(\"Trying curl method for {}\".format(url[:50]))\n",
    "    content = get_html_curl(url, domain_name = domain_name)\n",
    "    if content:\n",
    "        print(\"Curl success!\")\n",
    "        return content\n",
    "    \n",
    "    # If both methods fail, return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab252bb",
   "metadata": {},
   "source": [
    "---\n",
    "### **Parse HTTP document & Extract information**\n",
    "\n",
    "This step is responsible for parsing http response, which can help more easily extract information from document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1553f8",
   "metadata": {},
   "source": [
    "**Parse HTTP document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html_text):\n",
    "    '''\n",
    "    This function works for parsing http document\n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    return BeautifulSoup(html_text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc5f45",
   "metadata": {},
   "source": [
    "**Extract information**\n",
    "\n",
    "This step is responsible for extract information in a `BeautifulSoup` object, which is an object after parsing the http document. This is done by `select(selector:str)`. However, because the extraction depends on selectors, while the selectors are different with each website. In this document, I will choose articles on the website https://batdongsan.com.vn/tintuc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(html_parse: BeautifulSoup) -> list:\n",
    "    '''\n",
    "    Function gets the title of the articles after parsing to BeautifulSoup object\n",
    "    --> Return : List of title\n",
    "    '''\n",
    "    return [html_parse.title.text]\n",
    "\n",
    "def get_publisher(html_parse: BeautifulSoup) -> list:\n",
    "    '''\n",
    "    Function gets the publisher, or list of publisher of the document\n",
    "    Publisher is a person or an organization publishing the article, not the author\n",
    "    --> Return : List of publisher\n",
    "    '''\n",
    "    selector = '#__next main > div.ArticlePageTemplate_articlePageContainer__wcRoZ.container > div:nth-child(2) > div > div > div > div.AuthorInfo_authorName__m9KD3 > a'\n",
    "    publisher_ls = html_parse.select(selector)\n",
    "    if len(publisher_ls):\n",
    "        return [publisher.text for publisher in publisher_ls]\n",
    "    return None\n",
    "\n",
    "def get_latest_update(html_parse: BeautifulSoup) -> list:\n",
    "    '''\n",
    "    Function gets the latest update time\n",
    "    --> Return: list of update time\n",
    "    '''\n",
    "    selector = '#__next main > div.ArticlePageTemplate_articlePageContainer__wcRoZ.container > div:nth-child(2) > div > div > div > div.AuthorInfo_postDate__UTKIr'\n",
    "    update_time_ls = html_parse.select(selector)\n",
    "\n",
    "    if len(update_time_ls):\n",
    "        return [update_time.text for update_time in update_time_ls]\n",
    "    return None\n",
    "\n",
    "def get_content(html_parse: BeautifulSoup) -> dict:\n",
    "    '''\n",
    "    Function gets the article's content\n",
    "    --> Return: dict: {\n",
    "        \"content\"       : str,\n",
    "        \"author\"        : str,\n",
    "        \"source\"        : str,\n",
    "        \"publish time\"  : str,\n",
    "        \"link source\"   : str\n",
    "    }\n",
    "    '''\n",
    "    selector = '#__next main > div.ArticlePageTemplate_articlePageContainer__wcRoZ.container > div:nth-child(4) > div.col-xl-8.col-lg-8.col-md-12.col-12 > article > div:nth-child(1) > *'\n",
    "    content_list = html_parse(selector)\n",
    "\n",
    "    if len(content_list):\n",
    "        full_content = ''\n",
    "        accept_tag = ['p', 'h1', 'h2', 'h3', 'h4', 'h5'] # list of acceptable tags, this is for eleminating the figure\n",
    "        end_main_content = ['——']                        # list of signals which indicate that the main content ends\n",
    "\n",
    "        author = 'Unknown'\n",
    "        source = 'Unknown'\n",
    "        publish_time = 'Unknown'\n",
    "        source_link = 'Unknown'\n",
    "\n",
    "        # Extract full content, full content ends by the line '——'\n",
    "        for i in range(len(content_list)):\n",
    "            if content_list[i].name in accept_tag and content_list[i].text not in end_main_content:\n",
    "                full_content += content_list[i].text + \"\\n\\n\"\n",
    "        \n",
    "        # Extract author and original article: \n",
    "        # after main content, the information of author and the original article is shown in (i+1)-th element\n",
    "        # ( Because the i-th element is the line '——')\n",
    "        if i+1 < len(content_list)-1: \n",
    "            # format of information is:\n",
    "            # <p>Tác giả:...<br>Nguồn:...<br>\n",
    "            # split it into separate parts: ['Tác giả:...', <br>, 'Nguồn:...', <br>,...] by method `children`\n",
    "            for child in content_list[i+1].children:\n",
    "                if child.name != \"br\":\n",
    "                    separate_index = child.text.find(\":\")+1\n",
    "                    info = child.text[separate_index:]\n",
    "                    format_text = \" \".join(info.split())\n",
    "                    if 'tác giả' in child.text.lower():\n",
    "                        author = format_text\n",
    "                    if 'nguồn tin' in child.text.lower():\n",
    "                        source = format_text\n",
    "                    if 'thời gian' in child.text.lower():\n",
    "                        publish_time = format_text\n",
    "                    if 'link' in child.text.lower():\n",
    "                        source_link = format_text\n",
    "\n",
    "            return {\n",
    "                \"content\": full_content,\n",
    "                \"author\" : author,\n",
    "                \"source\" : source,\n",
    "                \"publish time\": publish_time,\n",
    "                \"source link\": source_link\n",
    "            }\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e8a0e",
   "metadata": {},
   "source": [
    "**[FULL]: Getting information of a url link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_information(url: str) -> dict:\n",
    "    '''\n",
    "    Function gets full information of an article\n",
    "    --> Return: dict:\n",
    "        {\n",
    "            \"title\"                 : str,\n",
    "            \"publisher\"             : str,\n",
    "            \"lastest update time\"   : str,\n",
    "            \"content\"               : str,\n",
    "            \"author\"                : str,\n",
    "            \"source\"                : str,\n",
    "            \"publish time\"          : str,\n",
    "            \"link source\"           : str\n",
    "        }\n",
    "    '''\n",
    "    print(\"\\rCrawling {}\".format(url), end = \"\")\n",
    "\n",
    "    # Send request and receive html text\n",
    "    html_text = get_html_combine(url)\n",
    "    # parsing html\n",
    "    html_parse = parse_html(html_text)\n",
    "    # Get title\n",
    "    print(\"\\r       Getting title...\", end = \"\")\n",
    "    title = get_title(html_parse)\n",
    "    if title:\n",
    "        title = title[0]\n",
    "    else: \n",
    "        title = 'Unknown'\n",
    "        print(\"     |X| Cannot get the title !!!\")\n",
    "        \n",
    "    # Get publisher\n",
    "    print(\"\\r       Getting publisher...\", end = \"\")\n",
    "    publiser = get_publisher(html_parse)\n",
    "    if publiser:\n",
    "        publiser = \", \".join(publiser)\n",
    "    else:\n",
    "        print((\"      |X| Cannot get the publisher !!!\"))\n",
    "        publiser = 'Unknown'\n",
    "\n",
    "    # Get latest update time\n",
    "    print(\"\\r       Getting latest update time...\", end=\"\")\n",
    "    latest_update_time = get_latest_update(html_parse)\n",
    "    if latest_update_time:\n",
    "        latest_update_time = \" \".join(latest_update_time)\n",
    "    else:\n",
    "        print((\"    |X| Cannot get the latest update time !!!\"))\n",
    "        publiser = 'Unknown'\n",
    "\n",
    "    # Get content, author, source, publish time, source link\n",
    "    result = get_content(html_parse)\n",
    "    print(\"\\r       Getting content, author, source, publish time...\", end = \"\")\n",
    "    if result:\n",
    "        result[\"title\"] = title\n",
    "        result[\"publisher\"] = publiser\n",
    "        result[\"latest update time\"] = latest_update_time\n",
    "        print(\"\\r       Finish crawling!\", end = \"\")\n",
    "        \n",
    "    else:\n",
    "        print(\"    |X| Cannot get the content, author, source, publish time!!!\")\n",
    "        return {\n",
    "            \"title\"                 : title,\n",
    "            \"publisher\"             : publiser,\n",
    "            \"lastest update time\"   : latest_update_time,\n",
    "            \"content\"               : 'Unknown',\n",
    "            \"author\"                : 'Unknown',\n",
    "            \"source\"                : 'Unknown',\n",
    "            \"publish time\"          : 'Unknown',\n",
    "            \"link source\"           : 'Unknown'\n",
    "        }\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "QUICK TEST\n",
    "'''\n",
    "test_url = 'https://batdongsan.com.vn/tin-tuc/maison-grand-buoc-ngoat-moi-tren-hanh-trinh-kien-tao-chuan-song-chuyen-gia-tai-sieu-cang-quoc-te-phu-my-847604'\n",
    "result = get_full_information(test_url)\n",
    "for key, value in result.items():\n",
    "    print(\"{} : {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078362e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
